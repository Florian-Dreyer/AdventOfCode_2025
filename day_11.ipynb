{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5e861c",
   "metadata": {},
   "source": [
    "# Solution for Puzzle x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3f8ec0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7971c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8838ebf9",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04f3ca7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resulting DataFrame ---\n",
      "  node              vertices\n",
      "0  zlo                 [tnx]\n",
      "1  cwi       [lct, vxu, was]\n",
      "2  agf            [dac, fob]\n",
      "3  uqn  [mik, ygx, yrg, zpj]\n",
      "4  saf                 [ktw]\n"
     ]
    }
   ],
   "source": [
    "def read_and_parse_irregular_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads an irregularly formatted text file line by line, parses the data \n",
    "    blocks using string splitting, and returns a consolidated Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the input file (e.g., 'input/test.csv').\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the parsed data.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise ValueError(f\"Error: File not found at '{file_path}'. Please check the path.\")\n",
    "        \n",
    "    raw_lines = []\n",
    "    try:\n",
    "        # Read the entire file content into a list of strings\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            # Strip whitespace and filter out empty lines\n",
    "            raw_lines = [line.strip() for line in file if line.strip()]\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"An error occurred while reading the file: {e}\")\n",
    "\n",
    "    parsed_data = []\n",
    "\n",
    "    # Parse each line string\n",
    "    for line in raw_lines:\n",
    "\n",
    "        # Split the line by spaces to separate the data blocks\n",
    "        parts = line.split(\": \")\n",
    "        \n",
    "        # Check if the line contains enough data blocks\n",
    "        if len(parts) < 2:\n",
    "            print(f\"Skipping line due to insufficient data: {line}\")\n",
    "            continue\n",
    "\n",
    "        node = parts[0]\n",
    "        vertices = parts[1].split(\" \")\n",
    "        \n",
    "        # Prepare the data dictionary for the row\n",
    "        row_dict = {'node': node, 'vertices': vertices}\n",
    "        \n",
    "        parsed_data.append(row_dict)\n",
    "\n",
    "    # 3. Create the final DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(parsed_data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "file_to_process = 'input/day_11.csv'\n",
    "input = read_and_parse_irregular_data(file_to_process)\n",
    "\n",
    "print(\"\\n--- Resulting DataFrame ---\")\n",
    "print(input.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab3ede1",
   "metadata": {},
   "source": [
    "## Part One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71138e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 497 possible paths.\n"
     ]
    }
   ],
   "source": [
    "# Building graph datastructure\n",
    "graph = {}\n",
    "for _, row in input.iterrows():\n",
    "    graph[row[\"node\"]] = row[\"vertices\"]\n",
    "\n",
    "# Traversing graph starting from you node\n",
    "paths = set()\n",
    "stack = [(\"you\", [])]\n",
    "while stack:\n",
    "    curr_node, visited = stack.pop(0)\n",
    "    visited.append(curr_node)\n",
    "    for vertice in graph[curr_node]:\n",
    "        if vertice == \"out\":\n",
    "            paths.add(tuple(visited))\n",
    "        elif vertice not in visited:\n",
    "            stack.append((vertice, visited.copy()))\n",
    "\n",
    "print(f\"There are {len(paths)} possible paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7567353c",
   "metadata": {},
   "source": [
    "## Part Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faec99f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 358564784931864 possible paths.\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "def dfs_memoized(graph):\n",
    "    \"\"\"\n",
    "    Finds the number of paths from 'svr' to 'out' that visit both 'dac' and 'fft'.\n",
    "\n",
    "    Args:\n",
    "        graph (dict): A dictionary where keys are device nodes and values are\n",
    "                      a list of devices they connect to (outgoing connections).\n",
    "    \"\"\"\n",
    "\n",
    "    # Memoized DFS function\n",
    "    @lru_cache(None) \n",
    "    def count_paths(start, end):\n",
    "        \"\"\"\n",
    "        Counts the number of unique directed paths from 'start' to 'end' in the graph.\n",
    "        \"\"\"\n",
    "        if start == end:\n",
    "            return 1\n",
    "\n",
    "        path_count = 0\n",
    "        \n",
    "        # Recurse on all neighbors\n",
    "        for neighbor in graph.get(start, []):\n",
    "            path_count += count_paths(neighbor, end)\n",
    "\n",
    "        return path_count\n",
    "\n",
    "    # Define the key nodes required by the puzzle\n",
    "    START_NODE = 'svr'\n",
    "    END_NODE = 'out'\n",
    "    NODE_A = 'fft'\n",
    "    NODE_B = 'dac'\n",
    "\n",
    "    # Calculate paths for Case 1: svr -> ... -> NODE_A -> ... -> NODE_B -> ... -> out\n",
    "    paths_p1 = count_paths(START_NODE, NODE_A)\n",
    "    paths_p2 = count_paths(NODE_A, NODE_B)\n",
    "    paths_p3 = count_paths(NODE_B, END_NODE)\n",
    "    count_case_1 = paths_p1 * paths_p2 * paths_p3\n",
    "    \n",
    "    # Calculate paths for Case 2: svr -> ... -> NODE_B -> ... -> NODE_A -> ... -> out\n",
    "    paths_p4 = count_paths(START_NODE, NODE_B)\n",
    "    paths_p5 = count_paths(NODE_B, NODE_A)\n",
    "    paths_p6 = count_paths(NODE_A, END_NODE)\n",
    "    count_case_2 = paths_p4 * paths_p5 * paths_p6\n",
    "\n",
    "    total_paths = count_case_1 + count_case_2\n",
    "    \n",
    "    count_paths.cache_clear() \n",
    "    \n",
    "    return total_paths\n",
    "\n",
    "print(f\"There are {dfs_memoized(graph)} possible paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c08e0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
